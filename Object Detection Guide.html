<html><head><title>github</title><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_2ywa23bw5rmy-1>li:before{content:"\0025cb  "}.lst-kix_r6e6dmud1v6l-3>li:before{content:"\0025cf  "}.lst-kix_vok8m16hq5s7-6>li:before{content:"\0025cf  "}.lst-kix_dux43ze3q5i2-3>li:before{content:"\0025cf  "}ul.lst-kix_r6e6dmud1v6l-4{list-style-type:none}ul.lst-kix_r6e6dmud1v6l-3{list-style-type:none}ul.lst-kix_r6e6dmud1v6l-2{list-style-type:none}.lst-kix_3ffidz1k3e91-3>li:before{content:"\0025cf  "}ul.lst-kix_r6e6dmud1v6l-1{list-style-type:none}.lst-kix_r6e6dmud1v6l-8>li:before{content:"\0025a0  "}ul.lst-kix_r6e6dmud1v6l-8{list-style-type:none}ul.lst-kix_r6e6dmud1v6l-7{list-style-type:none}ul.lst-kix_r6e6dmud1v6l-6{list-style-type:none}.lst-kix_383gviumi7v0-4>li:before{content:"\0025cb  "}ul.lst-kix_r6e6dmud1v6l-5{list-style-type:none}.lst-kix_2ywa23bw5rmy-2>li:before{content:"\0025a0  "}ul.lst-kix_r6e6dmud1v6l-0{list-style-type:none}.lst-kix_r756ao8e80f0-8>li:before{content:"\0025a0  "}.lst-kix_vx7v1vagsftz-8>li:before{content:"\0025a0  "}.lst-kix_vk2hwid075ip-0>li:before{content:"\0025cf  "}.lst-kix_r6e6dmud1v6l-6>li:before{content:"\0025cf  "}.lst-kix_2ywa23bw5rmy-6>li:before{content:"\0025cf  "}.lst-kix_2ywa23bw5rmy-3>li:before{content:"\0025cf  "}.lst-kix_vx7v1vagsftz-0>li:before{content:"\0025cf  "}ul.lst-kix_383gviumi7v0-3{list-style-type:none}ul.lst-kix_383gviumi7v0-4{list-style-type:none}ul.lst-kix_vk2hwid075ip-8{list-style-type:none}ul.lst-kix_383gviumi7v0-1{list-style-type:none}ul.lst-kix_vk2hwid075ip-7{list-style-type:none}.lst-kix_2ywa23bw5rmy-7>li:before{content:"\0025cb  "}ul.lst-kix_383gviumi7v0-2{list-style-type:none}ul.lst-kix_vk2hwid075ip-6{list-style-type:none}ul.lst-kix_vk2hwid075ip-5{list-style-type:none}.lst-kix_2ywa23bw5rmy-8>li:before{content:"\0025a0  "}ul.lst-kix_383gviumi7v0-0{list-style-type:none}ul.lst-kix_vk2hwid075ip-4{list-style-type:none}.lst-kix_vk2hwid075ip-4>li:before{content:"\0025cb  "}ul.lst-kix_vk2hwid075ip-3{list-style-type:none}.lst-kix_vx7v1vagsftz-2>li:before{content:"\0025a0  "}ul.lst-kix_vok8m16hq5s7-8{list-style-type:none}ul.lst-kix_vk2hwid075ip-1{list-style-type:none}ul.lst-kix_vok8m16hq5s7-7{list-style-type:none}ul.lst-kix_vk2hwid075ip-2{list-style-type:none}.lst-kix_dux43ze3q5i2-0>li:before{content:"\0025cf  "}.lst-kix_vx7v1vagsftz-7>li:before{content:"\0025cb  "}ul.lst-kix_vok8m16hq5s7-6{list-style-type:none}ul.lst-kix_vok8m16hq5s7-5{list-style-type:none}ul.lst-kix_vk2hwid075ip-0{list-style-type:none}ul.lst-kix_vx7v1vagsftz-3{list-style-type:none}ul.lst-kix_383gviumi7v0-8{list-style-type:none}ul.lst-kix_vx7v1vagsftz-2{list-style-type:none}ul.lst-kix_383gviumi7v0-7{list-style-type:none}ul.lst-kix_vx7v1vagsftz-1{list-style-type:none}ul.lst-kix_383gviumi7v0-6{list-style-type:none}.lst-kix_r6e6dmud1v6l-7>li:before{content:"\0025cb  "}ul.lst-kix_vx7v1vagsftz-0{list-style-type:none}ul.lst-kix_383gviumi7v0-5{list-style-type:none}ul.lst-kix_vx7v1vagsftz-7{list-style-type:none}ul.lst-kix_vok8m16hq5s7-0{list-style-type:none}ul.lst-kix_vx7v1vagsftz-6{list-style-type:none}ul.lst-kix_vx7v1vagsftz-5{list-style-type:none}ul.lst-kix_vx7v1vagsftz-4{list-style-type:none}ul.lst-kix_vok8m16hq5s7-4{list-style-type:none}ul.lst-kix_vok8m16hq5s7-3{list-style-type:none}.lst-kix_vk2hwid075ip-5>li:before{content:"\0025a0  "}.lst-kix_r756ao8e80f0-5>li:before{content:"\0025a0  "}ul.lst-kix_vok8m16hq5s7-2{list-style-type:none}ul.lst-kix_vx7v1vagsftz-8{list-style-type:none}ul.lst-kix_vok8m16hq5s7-1{list-style-type:none}.lst-kix_dux43ze3q5i2-2>li:before{content:"\0025a0  "}.lst-kix_383gviumi7v0-8>li:before{content:"\0025a0  "}.lst-kix_r6e6dmud1v6l-0>li:before{content:"\0025cf  "}ul.lst-kix_dux43ze3q5i2-1{list-style-type:none}ul.lst-kix_dux43ze3q5i2-0{list-style-type:none}ul.lst-kix_2ywa23bw5rmy-0{list-style-type:none}.lst-kix_vk2hwid075ip-1>li:before{content:"\0025cb  "}.lst-kix_383gviumi7v0-6>li:before{content:"\0025cf  "}.lst-kix_3ffidz1k3e91-0>li:before{content:"\0025cf  "}ul.lst-kix_2ywa23bw5rmy-8{list-style-type:none}ul.lst-kix_2ywa23bw5rmy-7{list-style-type:none}ul.lst-kix_2ywa23bw5rmy-6{list-style-type:none}ul.lst-kix_2ywa23bw5rmy-5{list-style-type:none}.lst-kix_vk2hwid075ip-8>li:before{content:"\0025a0  "}ul.lst-kix_2ywa23bw5rmy-4{list-style-type:none}.lst-kix_dux43ze3q5i2-4>li:before{content:"\0025cb  "}.lst-kix_r6e6dmud1v6l-5>li:before{content:"\0025a0  "}ul.lst-kix_2ywa23bw5rmy-3{list-style-type:none}ul.lst-kix_2ywa23bw5rmy-2{list-style-type:none}ul.lst-kix_2ywa23bw5rmy-1{list-style-type:none}.lst-kix_383gviumi7v0-2>li:before{content:"\0025a0  "}.lst-kix_vk2hwid075ip-7>li:before{content:"\0025cb  "}.lst-kix_3ffidz1k3e91-7>li:before{content:"\0025cb  "}ul.lst-kix_dux43ze3q5i2-2{list-style-type:none}.lst-kix_r756ao8e80f0-1>li:before{content:"\0025cb  "}.lst-kix_vok8m16hq5s7-2>li:before{content:"\0025a0  "}ul.lst-kix_dux43ze3q5i2-3{list-style-type:none}ul.lst-kix_dux43ze3q5i2-4{list-style-type:none}ul.lst-kix_dux43ze3q5i2-5{list-style-type:none}ul.lst-kix_dux43ze3q5i2-6{list-style-type:none}ul.lst-kix_dux43ze3q5i2-7{list-style-type:none}ul.lst-kix_dux43ze3q5i2-8{list-style-type:none}.lst-kix_vok8m16hq5s7-4>li:before{content:"\0025cb  "}ul.lst-kix_3ffidz1k3e91-1{list-style-type:none}ul.lst-kix_3ffidz1k3e91-2{list-style-type:none}ul.lst-kix_3ffidz1k3e91-3{list-style-type:none}.lst-kix_383gviumi7v0-7>li:before{content:"\0025cb  "}ul.lst-kix_3ffidz1k3e91-4{list-style-type:none}ul.lst-kix_3ffidz1k3e91-5{list-style-type:none}ul.lst-kix_3ffidz1k3e91-6{list-style-type:none}ul.lst-kix_3ffidz1k3e91-7{list-style-type:none}.lst-kix_r756ao8e80f0-0>li:before{content:"\0025cf  "}ul.lst-kix_3ffidz1k3e91-8{list-style-type:none}.lst-kix_3ffidz1k3e91-1>li:before{content:"\0025cb  "}.lst-kix_vok8m16hq5s7-5>li:before{content:"\0025a0  "}.lst-kix_r6e6dmud1v6l-2>li:before{content:"\0025a0  "}ul.lst-kix_3ffidz1k3e91-0{list-style-type:none}.lst-kix_vok8m16hq5s7-0>li:before{content:"\0025cf  "}.lst-kix_r756ao8e80f0-3>li:before{content:"\0025cf  "}.lst-kix_383gviumi7v0-1>li:before{content:"\0025cb  "}.lst-kix_vx7v1vagsftz-6>li:before{content:"\0025cf  "}.lst-kix_r756ao8e80f0-2>li:before{content:"\0025a0  "}.lst-kix_r756ao8e80f0-6>li:before{content:"\0025cf  "}.lst-kix_383gviumi7v0-0>li:before{content:"\0025cf  "}.lst-kix_383gviumi7v0-3>li:before{content:"\0025cf  "}.lst-kix_3ffidz1k3e91-8>li:before{content:"\0025a0  "}ul.lst-kix_r756ao8e80f0-0{list-style-type:none}ul.lst-kix_r756ao8e80f0-1{list-style-type:none}ul.lst-kix_r756ao8e80f0-2{list-style-type:none}ul.lst-kix_r756ao8e80f0-3{list-style-type:none}ul.lst-kix_r756ao8e80f0-4{list-style-type:none}ul.lst-kix_r756ao8e80f0-5{list-style-type:none}ul.lst-kix_r756ao8e80f0-6{list-style-type:none}ul.lst-kix_r756ao8e80f0-7{list-style-type:none}ul.lst-kix_r756ao8e80f0-8{list-style-type:none}.lst-kix_383gviumi7v0-5>li:before{content:"\0025a0  "}.lst-kix_r6e6dmud1v6l-1>li:before{content:"\0025cb  "}.lst-kix_vk2hwid075ip-6>li:before{content:"\0025cf  "}.lst-kix_dux43ze3q5i2-1>li:before{content:"\0025cb  "}.lst-kix_2ywa23bw5rmy-5>li:before{content:"\0025a0  "}.lst-kix_vx7v1vagsftz-3>li:before{content:"\0025cf  "}.lst-kix_2ywa23bw5rmy-0>li:before{content:"\0025cf  "}.lst-kix_3ffidz1k3e91-6>li:before{content:"\0025cf  "}.lst-kix_dux43ze3q5i2-8>li:before{content:"\0025a0  "}.lst-kix_dux43ze3q5i2-5>li:before{content:"\0025a0  "}.lst-kix_vok8m16hq5s7-7>li:before{content:"\0025cb  "}.lst-kix_vx7v1vagsftz-5>li:before{content:"\0025a0  "}.lst-kix_vok8m16hq5s7-1>li:before{content:"\0025cb  "}.lst-kix_r756ao8e80f0-4>li:before{content:"\0025cb  "}.lst-kix_vx7v1vagsftz-1>li:before{content:"\0025cb  "}.lst-kix_vk2hwid075ip-2>li:before{content:"\0025a0  "}.lst-kix_2ywa23bw5rmy-4>li:before{content:"\0025cb  "}.lst-kix_dux43ze3q5i2-6>li:before{content:"\0025cf  "}.lst-kix_r756ao8e80f0-7>li:before{content:"\0025cb  "}.lst-kix_vk2hwid075ip-3>li:before{content:"\0025cf  "}.lst-kix_vok8m16hq5s7-3>li:before{content:"\0025cf  "}.lst-kix_3ffidz1k3e91-2>li:before{content:"\0025a0  "}.lst-kix_vok8m16hq5s7-8>li:before{content:"\0025a0  "}.lst-kix_r6e6dmud1v6l-4>li:before{content:"\0025cb  "}.lst-kix_3ffidz1k3e91-5>li:before{content:"\0025a0  "}.lst-kix_dux43ze3q5i2-7>li:before{content:"\0025cb  "}.lst-kix_3ffidz1k3e91-4>li:before{content:"\0025cb  "}.lst-kix_vx7v1vagsftz-4>li:before{content:"\0025cb  "}ol{margin:0;padding:0}.c3{widows:2;orphans:2;height:11pt;direction:ltr}.c4{widows:2;orphans:2;direction:ltr}.c12{max-width:468pt;background-color:#ffffff;padding:72pt 72pt 72pt 72pt}.c9{color:#1155cc;text-decoration:underline}.c6{color:inherit;text-decoration:inherit}.c2{font-style:italic;font-weight:bold}.c13{margin-left:72pt}.c8{text-align:center}.c1{font-size:9pt}.c15{margin-left:36pt}.c17{font-size:10pt}.c5{text-indent:36pt}.c7{background-color:#cccccc}.c11{font-size:24pt}.c14{line-height:1.7}.c16{font-style:italic}.c0{font-size:12pt}.c10{font-weight:bold}.title{widows:2;padding-top:0pt;line-height:1.15;orphans:2;text-align:left;color:#000000;font-size:21pt;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}.subtitle{widows:2;padding-top:0pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-style:italic;font-size:13pt;font-family:"Trebuchet MS";padding-bottom:10pt;page-break-after:avoid}li{color:#000000;font-size:11pt;font-family:"Arial"}p{color:#000000;font-size:11pt;margin:0;font-family:"Arial"}h1{widows:2;padding-top:10pt;line-height:1.15;orphans:2;text-align:left;color:#000000;font-size:16pt;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}h2{widows:2;padding-top:10pt;line-height:1.15;orphans:2;text-align:left;color:#000000;font-size:13pt;font-family:"Trebuchet MS";font-weight:bold;padding-bottom:0pt;page-break-after:avoid}h3{widows:2;padding-top:8pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-size:12pt;font-family:"Trebuchet MS";font-weight:bold;padding-bottom:0pt;page-break-after:avoid}h4{widows:2;padding-top:8pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-size:11pt;text-decoration:underline;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}h5{widows:2;padding-top:8pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-size:11pt;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}h6{widows:2;padding-top:8pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-style:italic;font-size:11pt;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}</style></head><body class="c12"><p class="c4"><span class="c11">General Object Detection Process with OpenCV 2.4.5-2.4.9</span><hr></p><p class="c4"><span class="c0 c10">1. First and foremost</span></p><p class="c3 c15"><span class="c0 c10"></span></p><p class="c4 c5"><span class="c0">You must decide on what type of object you wish to detect. If you&rsquo;re detecting a logo or emblem that has very strict characteristics to it (ex. The Seminoles emblem or McDonalds logo) you won&rsquo;t need as many positive image samples. In fact, you can probably generate a good prototype cascade with as few as 20-50 clear image samples. However, if you&#39;re trying to generate a production level cascade, anywhere from 500-3000 unique images would be much better.</span></p><p class="c3 c15 c5"><span class="c0"></span></p><p class="c4 c5"><span class="c0">If you&rsquo;re trying to detect something with more variance, such as license plates( I&rsquo;ll elaborate on that specific process in the next page), you&rsquo;ll need to determine a good method of chunking the data together visually(for internal features). For license plates, it&rsquo;s easier to get away with &lsquo;chunking&rsquo; the data together because license plates have very similar external forms(Minus Canada). </span></p><p class="c3 c15 c5"><span class="c0"></span></p><p class="c4 c5"><span class="c0">If you&rsquo;re trying to detect an object with more shape variance, such as a mug, you really have two options. Since most mugs have very similar shapes, you could use a lot of data from as many different mugs you can find. However, this can sometimes be a tricky trial and error process. </span><span class="c2 c0">Provided the external forms of the mugs are reasonably similar in shape and dimension</span><span class="c0">, you would first group them by form as much as you can. Then, try grouping them by similar internal features. The order of this data as it goes into the trainer won&rsquo;t matter at all, however the amount will. Try and keep each group to similar percentages in the data set.</span></p><p class="c3 c15 c5"><span class="c0"></span></p><p class="c4 c5"><span class="c0">The other option would be to generate individual cascades that are more accurate for each individual type of mug. For example, one for short stout classic coffee mugs and one for taller thermos mugs. Both objects have very different forms, and both are technically mugs. This process will help you establish a much more accurate classifier, but it will take significantly more time to produce.</span></p><p class="c3 c5 c15"><span class="c0"></span></p><p class="c4 c5"><span class="c0">There is one major hassle that must be dealt with, and that is</span><span class="c0 c10">&nbsp;orientation</span><span class="c0">. If you train the computer to recognise a mug sitting upright on a table, it will only recognize it from that orientation. There are two primary ways to get around this:</span></p><p class="c3 c15 c5"><span class="c0"></span></p><p class="c4 c8 c5"><span class="c0">1. </span><span class="c0 c10">Rotate the source image</span><span class="c0">: Generate one cascade and trying the cascade on differently oriented versions of the image. This works for two dimensional applications.</span></p><p class="c3 c8 c5"><span class="c0"></span></p><p class="c4 c8 c5"><span class="c0">2. </span><span class="c0 c10">Rotate the cascades: </span><span class="c0">Generate multiple cascades based on different rotations of the same image data. As you can imagine, it would be possible to design a series of classifiers to track an object in three dimensions, no matter the orientation. </span></p><p class="c3 c8 c5"><span class="c0"></span></p><p class="c4 c5"><span class="c0">Both methods involve using a loop on either an array of cascades, flips/variations of an image and may have performance costs, depending on the nature of the application.</span></p><p class="c3 c13 c5"><span class="c0"></span></p><p class="c4 c5"><span class="c16 c0">One big thing to note, </span><span class="c2 c0">chrome and reflective objects</span><span class="c0">. Object that are chrome colored or highly reflective will be more difficult to track because the internal features easily change based on the features of the surrounding area. On top of that, the changes produce very fine and easily readable edges by the the cascade generator. It&rsquo;s not aware that these are specular highlights, and will instantly treat them like an edge. (Try running a chrome or reflective object through a Canny Edge filter to visualize this). This issue can be (somewhat) eliminated or reduced by making sure to photograph the object using special filters to reduce the amount of reflection captured by the camera or by lighting the object in a special way that reduces the reflections.</span></p><p class="c3 c5"><span class="c0"></span></p><hr><p class="c3"><span class="c0"></span></p><p class="c3"><span class="c0 c10"></span></p><p class="c4"><span class="c0 c10">2. How to collect positive sample images</span></p><p class="c3"><span class="c0 c10"></span></p><p class="c4"><span class="c0 c10">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0">Positive sample images are images that contain the object you wish to create a cascade classifier for.</span></p><p class="c3 c13 c5"><span class="c0"></span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Positive images can be taken manually or they can be obtained online from various image search engines. Images should be very clear, and have little or no blur. They should all be taken from the same orientation that you wish the computer to recognize them in. Shadows ideally should be non-existent. Shadows can cause unneeded coarse edges that will throw your data training off. The same thing goes for highlights or bright spots. If Shadows/highlights are soft, and more like gradients, thats not quite as bad and won&rsquo;t likely produce an unintended edge. The more even the lighting on the object and it&rsquo;s background, the better. Furthermore, the background of the image should be as evenly colored as possible, and not the same color as the object. There should be definitive contrast between the background and the object, this will help the trainCascade Algorithm determine the shape of the object. </span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If taking pictures by hand, a cool trick I used was creating a grid over my LCD viewfinder on my camera in which I would line the object up in before taking the picture. This makes cropping easier, as we&rsquo;ll talk about next.</span></p><p class="c3"><span class="c0"></span></p><hr><p class="c3"><span class="c0"></span></p><p class="c3 c5 c13"><span class="c0"></span></p><p class="c4"><span class="c0 c10">3. Prepping positive samples for training</span></p><p class="c3"><span class="c0 c10"></span></p><p class="c4"><span class="c0 c10">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0">After you&rsquo;ve collected the sample images, you&rsquo;ll want to crop and format them. This has to be done by hand, but in some cases you can use (and I would highly recommend) ImageMagick. If you use the trick I mentioned above [patent pending ;)], you can calculate the proper parameter values for the crop functionality in ImageMagick and easily batch crop all of your images. This will save you a lot of time. Otherwise, you must manually go through and crop all of your images. Next, you must make sure that all images have the same width to height ratio. If you&#39;re using ImageMagick, you can use a command like this to do the whole set:</span></p><p class="c4 c8"><span class="c7 c0">mogrify -resize 300x100! *</span></p><p class="c4"><span class="c0 c7">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0">The &lsquo;!&rsquo; operator specifies to make each image this (300x100) exact resolution. This is important, if they&rsquo;re not all the same width to height ratio, when you generate the vector file you can set yourself up for errors later. Although the images do not need to be the same resolution (600x200 has the same ratio as 300x100), I have found it best to keep them the same resolution, normally down sampling to a reasonably small sized image (less than 300x300 pixel area). Smaller images have less details, and thus it&rsquo;s easier for the trainCascade algorithm to better determine the features. This doesn&rsquo;t mean that you can&rsquo;t use larger images, but it&rsquo;s better to keep them smaller generally. </span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Making the images grayscale is optional, but won&rsquo;t really change anything for your training. </span></p><p class="c3"><span class="c0"></span></p><hr><p class="c3"><span class="c0"></span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">4. Preparation of negative samples for training. </span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Negative images are images that do not contain the object you&rsquo;re seeking to track. In the resources section above, I&rsquo;ve included a bunch of links that have negative image data sets that can be pulled from and utilized for training. These do not have to be the same size, have the same depth, or have the same aspect ratio. </span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you&rsquo;re trying to increase the effectiveness of your cascade, it&rsquo;s advisable that you find images that contain similar objects/shapes to the one you&rsquo;re trying to track. This, in theory, should help decrease the amount of false positives.</span></p><p class="c3"><span class="c0"></span></p><hr><p class="c3"><span class="c0"></span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0 c10">5. Creating Samples</span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I originally followed an excellent article at </span><span class="c9 c0"><a class="c6" href="http://www.google.com/url?q=http%3A%2F%2Fcoding-robin.de%2F2013%2F07%2F22%2Ftrain-your-own-opencv-haar-classifier.html&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFwa9VoQlLdQuNQkVjpkfXsb5iTIg">coding-robin here</a></span><span class="c0">. You can download the </span><span class="c9 c0"><a class="c6" href="https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2FTaylorTheDeveloper%2Fopencv-haar-classifier-training&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFRRbmAfesE3PSv6MFnaKMJ36v6Rw">github repository here</a></span><span class="c0">. This will save you a good bit of time. Since his article covers most of the general process quite well, I&rsquo;ll go more in depth on some of the parameters for parts of the process. Please look over the article before continuing. </span></p><p class="c3"><span class="c0"></span></p><p class="c4 c14"><span class="c0">For opencv_createsamples: </span></p><p class="c4 c8 c14"><span class="c7 c16 c17">perl bin/createsamples.pl positives.txt negatives.txt samples 1500\<br> &nbsp;&quot;opencv_createsamples -bgcolor 0 -bgthresh 0 -maxxangle 1.1\<br> &nbsp;-maxyangle 1.1 maxzangle 0.5 -maxidev 40 -w 80 -h 40&quot;</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This runs a perl script developed by </span><span class="c0 c9"><a class="c6" href="http://www.google.com/url?q=http%3A%2F%2Fnote.sonots.com%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNG3APQzRYmlKvW5o1RyjglpliLNDA">Naotoshi Seo</a></span><span class="c0">(He&rsquo;s got some cool stuff on his site) to </span><span class="c0 c10">generate 1500 vector sample images</span><span class="c0">&nbsp;by automatically performing transformations on images from the positives.txt file onto images from the negatives.txt file. </span></p><p class="c3 c5"><span class="c2 c0"></span></p><p class="c4 c5"><span class="c2 c0">&lsquo;-bgcolor&rsquo;</span><span class="c0">&nbsp;denotes the color value which is to be treated as transparent. In this case, anything that&rsquo;s 100% black will be transparent. </span><span class="c2 c0">&lsquo;-bgthresh&rsquo;</span><span class="c0">&nbsp;specifies the amount of tolerance, thus providing a range of values which can be interpreted as transparent. If you have a uniformish background color, you can use this to help eliminate it, thus easing the work traincascade must do to establish the form of your object. </span></p><p class="c3 c5"><span class="c2 c0"></span></p><p class="c4 c5"><span class="c2 c0">&lsquo;-maxidev&rsquo;</span><span class="c0">&nbsp;specifies the maximum intensity of deviation of the positive samples over the negative image. &nbsp;</span><span class="c2 c0">&lsquo;-maxxangle&rsquo;,&lsquo;-maxyangle&rsquo;, and &lsquo;-maxzangle&rsquo; </span><span class="c0">each specify the maximum rotation angles in radians. &nbsp;</span><span class="c2 c0">&lsquo;-w&rsquo; and &lsquo;-h&rsquo; </span><span class="c0">are the width and height of the output vector size. These must be the same aspect ratio as your original image. You&rsquo;ll wanna keep these dimensions less than (100x100). Larger vectors take longer to process and contain more unneeded information. </span><span class="c0 c16">Keep in mind that you can generate larger vectors. </span><span class="c0">They will take longer to process, however.</span></p><p class="c3"><span class="c0"></span></p><p class="c3 c8"><span class="c0"></span></p><p class="c4 c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 300.00px; height: 102.00px;"><img alt="trg.jpg" src="images/image00.jpg" style="width: 300.00px; height: 202.50px; margin-left: 0.00px; margin-top: -11.50px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c1">What comes out looks like this. [ Image Source: </span><span class="c9 c1"><a class="c6" href="https://www.google.com/url?q=https%3A%2F%2Fwww.ibm.com%2Fdeveloperworks%2Fcommunity%2Fblogs%2FtheTechTrek%2Fentry%2Fopencv_haartraining_and_all_that_jazz%3Flang%3Den&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHAbbbImUq6jt0Od3WMq-zX9PawIw">IBM&rsquo;s Tinniam Ganesh</a></span><span class="c1">&nbsp;]</span></p><p class="c3"><span class="c0"></span></p><p class="c4 c5"><span class="c0">There are other parameters not used. </span><span class="c2 c0">&lsquo;-inv&rsquo;</span><span class="c0">&nbsp;which inverts the colors of an image. And</span><span class="c0 c2">&nbsp;&lsquo;-randinv&rsquo; </span><span class="c0">which randomly inverts the colors of an images. There are a few more which you can read about </span><span class="c9 c0"><a class="c6" href="http://www.google.com/url?q=http%3A%2F%2Fdocs.opencv.org%2Fdoc%2Fuser_guide%2Fug_traincascade.html%23positive-samples&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHae3j4qD0ub1H5fXdg5JC5vAimjA">here</a></span><span class="c0">. </span></p><p class="c3 c5"><span class="c0"></span></p><p class="c4 c5"><span class="c0">Next, following coding-robins guide you should compile Naotoshi Seo&rsquo;s mergevec program and then you&rsquo;ll be ready to move onto training a classifier.</span></p><p class="c3 c5"><span class="c0"></span></p><hr><p class="c3"><span class="c0"></span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0 c10">6. Running the Trainer</span></p><p class="c3"><span class="c0 c10"></span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OpenCV 2.4.9 is the latest stable version at the time of writing and it contains two machine learning algorithms for object training. </span><span class="c2 c0">opencv_haartraining</span><span class="c0">&nbsp;and </span><span class="c2 c0">opencv_traincascade</span><span class="c0">. Do not use opencv_haartraining, as it is out of date and limited. opencv_traincascade supports generating cascades based on </span><span class="c9 c0"><a class="c6" href="http://www.google.com/url?q=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FHaar-like_features&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGMR68nsukgm4kKN-2irBppbqAG7g">HAAR</a></span><span class="c0">, </span><span class="c9 c0"><a class="c6" href="http://www.google.com/url?q=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLocal_binary_patterns&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNE9rkZySoluTO7taV5cPq1Vta63xw">LBP</a></span><span class="c0">, and </span><span class="c9 c0"><a class="c6" href="http://www.google.com/url?q=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FHistogram_of_oriented_gradients&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHULVdF06UJeTJPAjla-D81T_Ej_w">HOG</a></span><span class="c0">&nbsp;features. </span></p><p class="c3 c14"><span class="c0"></span></p><p class="c4 c14"><span class="c0">For opencv_traincascade:</span></p><p class="c4 c8 c14"><span class="c17 c7 c16">opencv_traincascade -data classifier -vec samples.vec -bg negatives.txt\<br> &nbsp;-numStages 20 -minHitRate 0.999 -maxFalseAlarmRate 0.5 -numPos 1000\<br> &nbsp;-numNeg 600 -w 80 -h 40 -mode ALL -precalcValBufSize 1024\<br> &nbsp;-precalcIdxBufSize 1024</span></p><p class="c3"><span class="c0"></span></p><p class="c3"><span class="c11"></span></p></body></html>